# -*- coding: utf-8 -*-
"""Tweet_Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Q7cMMt9jOqkwS3CaMMU9-47U7WDm1cD
"""

#### RUN FIRST TIME ONLY

##### THESE HASHTAGS MEAN EDITS NEEDED
##### EVNTUALLY RID OF SOME OF THESE PRINT STATEMENTS
##### NEED TO COMMENT ALL CODE, incorporate in comments and code what i did and said in old code/comments?

import numpy as np
import math as math
import pandas as pd
import seaborn as sns
import scipy as sp
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from numpy import array
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn import datasets, neighbors
from sklearn import linear_model
from matplotlib import pyplot as plt
from pandas import DataFrame
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import svm



#### RUN FIRST TIME ONLY
from google.colab import drive
drive.mount('/content/drive')

#### RUN FIRST TIME ONLY
# Retreive Original "Base" Tweet Data (to be used for training, validating, and testing) from a CSV file and convert into a Data Frame
!ls /content/drive/Shareddrives/ECEN\ 403/PulledTweets/Train.csv
df_og_data = pd.read_csv('/content/drive/Shareddrives/ECEN 403/PulledTweets/Train.csv')

#### RUN FIRST TIME ONLY
df_og_data = df_og_data.dropna()      # Rid any empty columns from the Orginal Tweet Data Frame
df_og_data.info()

#### RUN FIRST TIME ONLY
supervised_threat_labels_df_og_data = df_og_data["threat"]     # retreive the supervised labelling of whether tweets in the Tweet Orginial Data Set are threats or not

#### RUN FIRST TIME ONLY
tweet_df_og_data = df_og_data["tweet"]                                            # we are only interested in the tweet contents in the original tweet data frame
count_vect_tweet_df_og_data = CountVectorizer(min_df=1)                        # sparse csr matrix created to encode tweet contents into numbers in format "(tweet #, unique word #) Number of times unique word # appeared in tweet #"
tweet_csr_og_data = count_vect_tweet_df_og_data.fit_transform(tweet_df_og_data)      # sparse csr matrix only encodes the values that are non zero in order to make the array smaller                        
tweet_unique_word_count_og_data = tweet_csr_og_data.shape[1]                           # the number of unique words in the orginal tweet data set is defined by the # of columns in the csr matrix
print(tweet_csr_og_data)
tweet_unique_word_list_og_data = count_vect_tweet_df_og_data.get_feature_names()       # collect a list of the unique words in the original tweet data set
print(tweet_unique_word_list_og_data)

#### RUN FIRST TIME ONLY
X_train, X_test, y_train, y_test = train_test_split(tweet_csr_og_data, supervised_threat_labels_df_og_data, test_size=0.40, random_state=42)    # Use 60% of Tweet Original Data Set for training each ML model
X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.50, random_state=42)                                            # Use 20% of Tweet Orginal Data Set for validation of each ML model and the last 20% for testing between ML models to see which one is best

#### RUN FIRST TIME ONLY
# Validation using Random Forest Classifier
RF = RandomForestClassifier()                                                   # Random Forest Classifier Algorithim
RF.fit(X_train,y_train)
y_pred_RF = RF.predict(X_valid)
print('accuracy =', accuracy_score(y_valid, y_pred_RF))                         # percent accuracy of ML RF algorithim using validation data
print(classification_report(y_valid, y_pred_RF))
confusion_matrix_RF = confusion_matrix(y_valid, y_pred_RF)                      # confusion matrix of ML RF algorithim using validation data
print('confusion matrix')
print(confusion_matrix_RF)
amount_incorrect_RF = confusion_matrix_RF[0][1] + confusion_matrix_RF[1][0]      # number of incorrect predictions is the top right and bottom left of the confusion matrix

#### RUN FIRST TIME ONLY
# Validation use Support Vector Machines
SVM = svm.SVC()                                                          # Support Vector Machine Algorithim
SVM.fit(X_train, y_train)
y_pred_SVM = SVM.predict(X_valid)
print('accuracy =', accuracy_score(y_valid, y_pred_SVM))                 # percent accuracy of ML SVM algorithim using validation data
print(classification_report(y_valid, y_pred_SVM))
confusion_matrix_SVM = confusion_matrix(y_valid, y_pred_SVM)             # confusion matrix of ML SVM algorithim using validation data
print('confusion matrix')
print(confusion_matrix_SVM)
amount_incorrect_SVM = confusion_matrix_SVM[0][1] + confusion_matrix_SVM[1][0]  # number of incorrect predictions is the top right and bottom left of the confusion matrix

#### RUN FIRST TIME ONLY
# Testing of best performing model #### ADD MORE MODELS ABOVE SO CAN TEST MORE HERE?
if (amount_incorrect_RF <= amount_incorrect_SVM):       # ML model that performed the best on the validation data is tested on the test data. This is done by comparing which model got the least incorrect
  y_pred = RF.predict(X_test)
elif (amount_incorrect_SVM > amount_incorrect_RF):
  y_pred = SVM.predict(X_test)
final_accuracy_score = accuracy_score(y_test, y_pred)   # percent accuracy of ML using testing data
print('accuracy =', final_accuracy_score)
print(classification_report(y_test, y_pred))
print('confusion matrix')
print(confusion_matrix(y_test, y_pred))                 # confusion matrix of ML using testing data

#### !!!!! RUN EVERY TIME, CHANGE FILE NAME
# Retreive Live Tweet Data from a CSV file and convert into a Data Frame
!ls /content/drive/Shareddrives/ECEN\ 403/PulledTweets/Test7.csv
df_live_data = pd.read_csv('/content/drive/Shareddrives/ECEN 403/PulledTweets/Test7.csv')

#### !!!!! RUN EVERY TIME
df_live_data = df_live_data.dropna()                     # Rid any empty columns from the Live Tweet Data Frame
df_live_data.info()

tweet_df_live_data = df_live_data["tweet"]                                                   # Content of the live tweet
time_df_live_data = df_live_data["created_at"]                                               # Time the live tweet was tweeted
lat_df_live_data = df_live_data["lat"]                                                       # Latitude of the live tweet      #### WOULD THIS BE THE LOCATION FORMAT JULIO GIVES ME?
long_df_live_data = df_live_data["long"]                                                     # Longitude of the live tweet     #### WOULD THIS BE THE LOCATION FORMAT JULIO GIVES ME?
count_vect_tweet_df_live_data = CountVectorizer(min_df=1)
tweet_csr_live_data = count_vect_tweet_df_live_data.fit_transform(tweet_df_live_data)
tweet_unique_word_count_live_data = tweet_csr_live_data.shape[1]                       # The amount of unique words in the live tweet data set
print(tweet_csr_live_data)
tweet_unique_word_list_live_data = count_vect_tweet_df_live_data.get_feature_names()       # collect a list of the unique words in the live tweet data set
print(tweet_unique_word_list_live_data)

# In order to use the trained ML, Resize the Tweet Live Data Set to be the same size as the Tweet Original Data Set that the ML was trained on
tweet_array_live_data_resized = [[0 for x in range(tweet_unique_word_count_og_data)] for i in range(1)]    # Initialize an array for the Tweet Live Data Set of the same size as the Tweet Original Data Set

tweet_array_live_data = tweet_csr_live_data.toarray()                                                      # In order to resize, Convert the Tweet Live Data Set to an array

for i in range(len(tweet_array_live_data[0])):                                                             # Resize the Tweet Live Data Set       
  for j in range(len(tweet_unique_word_list_og_data)):                                                     # OVERIDE UNIQUE WORDS IN WORD COUNTS OF tweet_array_live_data_resized THAT ALREADY EXIST IN THE WORD LIST OF THE TWEET ORIGINAL DATA SET, and IF WORDS FROM WORD LIST OF THE TWEET LIVE DATA SET ARENT IN THE WORD LIST OF THE TWEET ORIGINAL DATA SET, THEN DONT INCLUDE THOSE UNIQUE WORDS IN WORD COUNTS OF tweet_array_live_data_resized OR ELSE IT WILL MESS UP THE ML AS THE ML WAS NEVER TRAINED ON THOSE UNIQUE WORDS (maybe??) 
    if tweet_unique_word_list_live_data[i] == tweet_unique_word_list_og_data[j]:                       
      tweet_array_live_data_resized[0][j] = tweet_array_live_data[0][i]

print(tweet_array_live_data_resized)
tweet_csr_live_data = sp.sparse.csr_matrix(tweet_array_live_data_resized)                                  # Convert the Tweet Live Data Set back to a csr matrix in order to input it into the ML for prediction
print(tweet_csr_live_data.shape)
live_pred = RF.predict(tweet_csr_live_data)                                                                # ML live tweet prediction
                                        
print(live_pred)

#### RUN FIRST TIME ONLY / RUN WHENEVER WANT TO RESET LIVE TWEET POOL
# PROBABILITY CALCULATION SET UP
probability = 0.1                                                        # probability of tweet being a threat, starts at 10% at base probability
time_array_live_data = [['' for x in range(1)] for i in range(1)]        # array containing times of live tweets
location_array_live_data = [[0 for x in range(2)] for i in range(1)]    # array containing locations (lat, long) of live tweets           #### WOULD THIS BE THE LOCATION FORMAT JULIO GIVES ME?
current_latlong_array_live_data = [0 for x in range(2)]                  # array containing (lat, long) of current live tweet

#### !!!!! RUN EVERY TIME
# PROBABILITY CALCULATION                  ##### EVENTUALLY MAKE PROBABILITIES MORE ACCURATE WITHIN A SPECTRUM INSTEAD OF CUT OFFS AT 50 MILES 25 MILES HOUR MINUTE ETC

if live_pred == 1.:                                                    # only run probability calculation if the tweet was a threat

  # TIME PROBABILITY CALCULATION
  time_array_live_data.append(time_df_live_data[0])                    # add current threatening live data tweet time to array of all threatening live data tweet times pulled so far

  for i in range(len(time_array_live_data)):                           # compare the times between any two threatening live tweets pulled                                                     
    for j in range(len(time_array_live_data)):
      if j > i:                                                       
        if (time_array_live_data[i][0:10] == time_array_live_data[j][0:10]):     # check if the days are the same between the tweets, multiply by 10% prob
          probability *= 1.1
          if (time_array_live_data[i][11:13] == time_array_live_data[j][11:13]):   # check if the hours are the same between the tweets, multiply by 20% prob
            probability *= 1.2
            if (time_array_live_data[i][14:16] == time_array_live_data[j][14:16]):   # check if the minutes are the same between the tweets, multiply by 20% prob
              probability *= 1.2

  # LOCATION PROBABILITY CALCULATION
  current_latlong_array_live_data[0] = float(lat_df_live_data[0])       # Convert data frame latitude and longitude into numbers that can be used for location distance calculations
  current_latlong_array_live_data[1] = float(long_df_live_data[0])

  location_array_live_data.append(current_latlong_array_live_data)         # add current threatening live data tweet location to array of all threatening live data tweet locations pulled so far

  for i in range(len(location_array_live_data)):                           # compare the distance between the locations of any two threatening live tweets pulled                                                     
    for j in range(len(location_array_live_data)):
      if j > i and i != 0 and j!= 0:
        distance = math.sqrt(((abs(location_array_live_data[i][0])-abs(location_array_live_data[j][0]))*69.2)**2 + ((abs(location_array_live_data[i][1])-abs(location_array_live_data[j][1]))*69.172)**2)      # Calculate the distance between two lat,long by using the distance formula and the fact that 1 degree of latitude is 69.2 miles and 1 degree of longitude is 69.172 miles                                               
        if distance <= 50.:     # check if the tweets are within 50 miles between the tweets, multiply by 10% prob
          probability *= 1.1
          if distance <= 25.:   # check if the tweets are within 25 miles between the tweets, multiply by 20% prob
            probability *= 1.2
            if distance <= 10.:   # check if the tweets are within 10 miles between the tweets, multiply by 20% prob
              probability *= 1.2

  # IMAGE PROBABILITY CALCULATION
  for i in range(len(tweet_unique_word_list_og_data)):    # if tweet contains a twitter image, multiply by 10% prob
    if "t.co" in tweet_unique_word_list_og_data[i]:       #### ALG NOT SPLITTING WORD INTO T.CO?
      probability *= 1.1 

  if probability > 0.9:                                                  # max probability is 90 %
    probability = 0.9                             

# OUTPUT IF CURRENT TWEET IS A THREAT, PERCENT ACCURACY OF ML TEST THAT IT IS A THREAT, AND PROBABILITY IT IS A THREAT

if live_pred == 1.:                                                                                                # If live_pred was a 1, the tweet was a threat and the probability calculated is used
  print("Tweet:\n\n" + str(tweet_df_live_data[0]) + "\n\nThreat?\n\nYES with " + str(final_accuracy_score) + " % accuracy\n\nProbability?\n\n" + str(probability*100) + " %\n\n----------------\n\n")
else:                                                                                                              # If live_pred was not a 1, the tweet was not a threat and the probability is not used    
  print("Tweet\n\n" + str(tweet_df_live_data[0]) + "\n\nThreat?\n\nNO with " + str(final_accuracy_score) + " % accuracy\n\n----------------\n\n")


probability = 0.1      # Probability reset to base probability for the next tweet